{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2332a61e",
   "metadata": {},
   "source": [
    "<!--TABLE OF CONTENTS-->\n",
    "Contents:\n",
    "- [Calculation de nutriscore](#Calculation-de-nutriscore)\n",
    "- [Partie 0: Import de données](#Partie-0:-Import-de-données)\n",
    "- [Partie 1: Analyse de Composantes Principales](#Partie-1:-Analyse-de-Composantes-Principales)\n",
    "  - [Objectif : réduire les dimensions , en trouvant les composants qui explique le plus de variance](#Objectif-:-réduire-les-dimensions-,-en-trouvant-les-composants-qui-explique-le-plus-de-variance)\n",
    "    - [PCA étape 1 - selection des colonnes à prendre en compte dans l'ACP](#PCA-étape-1---selection-des-colonnes-à-prendre-en-compte-dans-l'ACP)\n",
    "    - [PCA étape 2 - Préparation des données sélectionnés pour l'ACP](#PCA-étape-2---Préparation-des-données-sélectionnés-pour-l'ACP)\n",
    "      - [2a: éliminer les outliers:](#2a:-éliminer-les-outliers:)\n",
    "      - [2b: éliminer les NaN:](#2b:-éliminer-les-NaN:)\n",
    "    - [PCA étape 3  -  standardise (centrage et réduction d'echelle)](#PCA-étape-3-----standardise-(centrage-et-réduction-d'echelle))\n",
    "  - [Additives](#Additives)\n",
    "    - [Scree plot](#Scree-plot)\n",
    "- [Cercle des corrélations](#Cercle-des-corrélations)\n",
    "- [Partie 2: Clustering via K Nearest Neighbours](#Partie-2:-Clustering-via-K-Nearest-Neighbours)\n",
    "    - [KNN step 1 - Divide Data Into Features and Labels](#KNN-step-1---Divide-Data-Into-Features-and-Labels)\n",
    "- [Partie 3 : ANOVA](#Partie-3-:-ANOVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Calculation de nutriscore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "palette = sns.color_palette(\"bright\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 0: Import de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données\n",
    "RAW_DATA_FILENAME='en.openfoodfacts.org.products.csv'\n",
    "RAW_DATA_DICT='raw_data_dict.csv'\n",
    "SAMPLE_DATA_FILENAME='data_sample100k.csv' # 100,000 registres\n",
    "CLEAN_DATA_FILENAME='openfoodfacts_cleaned.csv'\n",
    "CLEAN_DATA_DICT='clean_data_dict.csv'\n",
    "# Données supplémentaires\n",
    "ADDITIVES_DETAIL='additives_detail.csv'\n",
    "ADDITIVES_COUNT='additives_count.csv'\n",
    "\n",
    "DATA_FOLDER = '../data/raw'\n",
    "OUT_FOLDER = '../data/out'\n",
    "IMAGE_FOLDER = 'images'\n",
    "\n",
    "os_path_join = lambda folder, file: f'{folder}/{file}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colonnes de dates : ['created_t', 'created_datetime', 'last_modified_t', 'last_modified_datetime']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 'object',\n",
       " 'code': 'object',\n",
       " 'url': 'object',\n",
       " 'creator': 'object',\n",
       " 'product_name': 'object',\n",
       " 'generic_name': 'object',\n",
       " 'quantity': 'object',\n",
       " 'packaging': 'object',\n",
       " 'packaging_tags': 'object',\n",
       " 'brands': 'object',\n",
       " 'brands_tags': 'object',\n",
       " 'categories': 'object',\n",
       " 'categories_tags': 'object',\n",
       " 'categories_en': 'object',\n",
       " 'origins': 'object',\n",
       " 'origins_tags': 'object',\n",
       " 'origins_en': 'object',\n",
       " 'manufacturing_places': 'object',\n",
       " 'manufacturing_places_tags': 'object',\n",
       " 'labels': 'object',\n",
       " 'labels_tags': 'object',\n",
       " 'labels_en': 'object',\n",
       " 'emb_codes': 'object',\n",
       " 'emb_codes_tags': 'object',\n",
       " 'first_packaging_code_geo': 'object',\n",
       " 'cities_tags': 'object',\n",
       " 'purchase_places': 'object',\n",
       " 'stores': 'object',\n",
       " 'countries': 'object',\n",
       " 'countries_tags': 'object',\n",
       " 'countries_en': 'object',\n",
       " 'ingredients_text': 'object',\n",
       " 'allergens': 'object',\n",
       " 'traces': 'object',\n",
       " 'traces_tags': 'object',\n",
       " 'traces_en': 'object',\n",
       " 'serving_size': 'object',\n",
       " 'serving_quantity': 'object',\n",
       " 'additives_n': 'Int64',\n",
       " 'additives_tags': 'object',\n",
       " 'additives_en': 'object',\n",
       " 'ingredients_from_palm_oil_n': 'Int64',\n",
       " 'ingredients_that_may_be_from_palm_oil_n': 'Int64',\n",
       " 'ingredients_that_may_be_from_palm_oil_tags': 'object',\n",
       " 'nutriscore_score': 'float64',\n",
       " 'nutriscore_grade': 'object',\n",
       " 'nova_group': 'object',\n",
       " 'pnns_groups_1': 'object',\n",
       " 'pnns_groups_2': 'object',\n",
       " 'states': 'object',\n",
       " 'states_tags': 'object',\n",
       " 'states_en': 'object',\n",
       " 'brand_owner': 'object',\n",
       " 'ecoscore_score_fr': 'object',\n",
       " 'ecoscore_grade_fr': 'object',\n",
       " 'main_category': 'object',\n",
       " 'main_category_en': 'object',\n",
       " 'energy-kj_100g': 'float64',\n",
       " 'energy-kcal_100g': 'float64',\n",
       " 'energy_100g': 'float64',\n",
       " 'fat_100g': 'float64',\n",
       " 'saturated-fat_100g': 'float64',\n",
       " 'monounsaturated-fat_100g': 'float64',\n",
       " 'polyunsaturated-fat_100g': 'float64',\n",
       " 'trans-fat_100g': 'float64',\n",
       " 'cholesterol_100g': 'float64',\n",
       " 'carbohydrates_100g': 'float64',\n",
       " 'sugars_100g': 'float64',\n",
       " 'fiber_100g': 'float64',\n",
       " 'proteins_100g': 'float64',\n",
       " 'salt_100g': 'float64',\n",
       " 'sodium_100g': 'float64',\n",
       " 'vitamin-a_100g': 'float64',\n",
       " 'vitamin-c_100g': 'float64',\n",
       " 'potassium_100g': 'float64',\n",
       " 'calcium_100g': 'float64',\n",
       " 'iron_100g': 'float64',\n",
       " 'nutrition-score-fr_100g': 'float64'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DICT = os_path_join(OUT_FOLDER, CLEAN_DATA_DICT)\n",
    "data_dict_df=pd.read_csv(DATA_DICT,encoding='UTF-8',sep='\\t')\n",
    "# exclut les date_cols\n",
    "date_col_filter= data_dict_df['column'].str.endswith(('_datetime', '_t'))\n",
    "date_cols= data_dict_df[date_col_filter]['column'].tolist()\n",
    "print(f'colonnes de dates : {date_cols}')\n",
    "data_dict_df=data_dict_df[~date_col_filter]\n",
    "data_dict=dict(zip(data_dict_df['column'],data_dict_df['dtype'].values))\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file pour analyse exploratoire: ../data/out/openfoodfacts_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "CLEAN_DATA = os_path_join(OUT_FOLDER, CLEAN_DATA_FILENAME)\n",
    "print(f'data file pour analyse exploratoire: {CLEAN_DATA}')\n",
    "\n",
    "cleaned_data = pd.read_csv(CLEAN_DATA, sep='\\t', header=0,\n",
    "                      encoding='utf-8',\n",
    "                      dtype=data_dict,\n",
    "                      parse_dates=date_cols,\n",
    "                      infer_datetime_format=True,\n",
    "                      low_memory=True)  # Warning: dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1: Analyse de Composantes Principales\n",
    "\n",
    "- <https://www.datasklr.com/principal-component-analysis-and-factor-analysis/principal-component-analysis>\n",
    "- <https://jmausolf.github.io/code/pca_in_python/>\n",
    "- <https://cmdlinetips.com/2018/03/pca-example-in-python-with-scikit-learn/>\n",
    "\n",
    "## Objectif : réduire les dimensions , en trouvant les composants qui explique le plus de variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = cleaned_data[\"product_name\"] # ou data.index pour avoir les intitulés\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA étape 1 - selection des colonnes à prendre en compte dans l'ACP\n",
    "Sélectionner les variables quantitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99996 entries, 0 to 99995\n",
      "Data columns (total 25 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   additives_n                              35903 non-null  Int64  \n",
      " 1   ingredients_from_palm_oil_n              35903 non-null  Int64  \n",
      " 2   ingredients_that_may_be_from_palm_oil_n  35903 non-null  Int64  \n",
      " 3   nutriscore_score                         35867 non-null  float64\n",
      " 4   energy-kj_100g                           7129 non-null   float64\n",
      " 5   energy-kcal_100g                         76671 non-null  float64\n",
      " 6   energy_100g                              79271 non-null  float64\n",
      " 7   fat_100g                                 78863 non-null  float64\n",
      " 8   saturated-fat_100g                       76831 non-null  float64\n",
      " 9   monounsaturated-fat_100g                 2476 non-null   float64\n",
      " 10  polyunsaturated-fat_100g                 2484 non-null   float64\n",
      " 11  trans-fat_100g                           13260 non-null  float64\n",
      " 12  cholesterol_100g                         13428 non-null  float64\n",
      " 13  carbohydrates_100g                       78844 non-null  float64\n",
      " 14  sugars_100g                              78107 non-null  float64\n",
      " 15  fiber_100g                               24552 non-null  float64\n",
      " 16  proteins_100g                            78939 non-null  float64\n",
      " 17  salt_100g                                74653 non-null  float64\n",
      " 18  sodium_100g                              74653 non-null  float64\n",
      " 19  vitamin-a_100g                           10608 non-null  float64\n",
      " 20  vitamin-c_100g                           11027 non-null  float64\n",
      " 21  potassium_100g                           4693 non-null   float64\n",
      " 22  calcium_100g                             13529 non-null  float64\n",
      " 23  iron_100g                                13274 non-null  float64\n",
      " 24  nutrition-score-fr_100g                  35867 non-null  float64\n",
      "dtypes: Int64(3), float64(22)\n",
      "memory usage: 19.4 MB\n"
     ]
    }
   ],
   "source": [
    "def datetime_to_float(df):\n",
    "    date_cols= list(df.select_dtypes('datetime'))\n",
    "    for col in date_cols:\n",
    "        df[col]=df[col].astype('datetime64').view(np.int64).astype(np.float64)\n",
    "    return df\n",
    "\n",
    "def pca_select_vars(df,exclude_cols=[]):\n",
    "    return (df\n",
    "       .select_dtypes(include=['number'])\n",
    "       .drop(columns=exclude_cols)\n",
    "    )\n",
    "\n",
    "pca_data= cleaned_data.pipe(pca_select_vars)\n",
    "pca_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99996 entries, 0 to 99995\n",
      "Data columns (total 9 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   ingredients_from_palm_oil_n              35903 non-null  Int64  \n",
      " 1   ingredients_that_may_be_from_palm_oil_n  35903 non-null  Int64  \n",
      " 2   energy_100g                              79271 non-null  float64\n",
      " 3   fat_100g                                 78863 non-null  float64\n",
      " 4   saturated-fat_100g                       76831 non-null  float64\n",
      " 5   carbohydrates_100g                       78844 non-null  float64\n",
      " 6   sugars_100g                              78107 non-null  float64\n",
      " 7   proteins_100g                            78939 non-null  float64\n",
      " 8   sodium_100g                              74653 non-null  float64\n",
      "dtypes: Int64(2), float64(7)\n",
      "memory usage: 7.1 MB\n"
     ]
    }
   ],
   "source": [
    "pca_data1= pca_data.drop(columns=['vitamin-a_100g','vitamin-c_100g','calcium_100g',\n",
    "'iron_100g','energy-kcal_100g','energy-kj_100g','monounsaturated-fat_100g','polyunsaturated-fat_100g',\n",
    "'trans-fat_100g','cholesterol_100g','additives_n','salt_100g','fiber_100g','potassium_100g','nutriscore_score',\n",
    "'nutrition-score-fr_100g'])\n",
    "pca_data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99996, 58)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data=cleaned_data.select_dtypes(exclude=['number'])\n",
    "target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA étape 2 - Préparation des données sélectionnés pour l'ACP\n",
    "\n",
    "#### 2a: éliminer les outliers:\n",
    "  - pour réduire le risque de trop d'influence des outliers \n",
    "  - eliminer ou abs(Z-score) > 3  (pas bon pour des colonnes avec beaucoup de skew)\n",
    "  - eliminer les top 1% et bottom 1%\n",
    "\n",
    "#### 2b: éliminer les NaN:\n",
    "  - dropna() # pour ne reduire pas le variance d'un variable\n",
    "  - fillna(data_pca.mean()) # Il est fréquent de remplacer les valeurs inconnues par la moyenne de la variable\n",
    "  - drop(columns={colonnes moins importants qui contient des NaN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99996, 25)\n",
      "(99996, 9)\n",
      "drop_high_outliers, nb=4580\n",
      "(33864, 9)\n",
      "drop_low_outliers, nb=1\n",
      "(35902, 9)\n",
      "intersection of result\n",
      "(33863, 9)\n"
     ]
    }
   ],
   "source": [
    "def drop_high_outliers(df, subset=None, exclude=None, alpha=0.01):\n",
    "    ## elimine 1% des valeurs les plus grands \n",
    "    filtre = False\n",
    "    if subset is None: subset= list(df.select_dtypes('number'))\n",
    "    if not exclude is None:\n",
    "        subset = list(set(subset)-set(exclude))\n",
    "    for column in subset:\n",
    "        # min_val = df[column].quantile(alpha)\n",
    "        max_val = df[column].quantile(1-alpha)\n",
    "        # filtre &= df[column]>min_val\n",
    "        filtre |= df[column]>max_val\n",
    "    \n",
    "    nb=df[filtre].shape[0]\n",
    "    print(f'drop_high_outliers, nb={nb}') #', (subset={subset})')\n",
    "    return df[~filtre]    \n",
    "\n",
    "def drop_low_outliers(df, subset=None, exclude=None, alpha=0.01):\n",
    "    ## elimine 1% des valeurs les plus petits \n",
    "    filtre = False\n",
    "    if subset is None: subset= list(df.select_dtypes('number'))\n",
    "    if not exclude is None: subset = list(set(subset)-set(exclude))\n",
    "    for column in subset:\n",
    "        min_val = df[column].quantile(alpha)\n",
    "        # max_val = df[column].quantile(1-alpha)\n",
    "        filtre |= df[column]<min_val\n",
    "        # filtre &= df[column]<max_val\n",
    "\n",
    "    nb=df[filtre].shape[0]\n",
    "    print(f'drop_low_outliers, nb={nb}') #, (subset={subset})')\n",
    "    return df[~filtre]\n",
    "\n",
    "print(pca_data.shape)\n",
    "print(pca_data1.shape)\n",
    "pca_d1=pca_data1.pipe(drop_high_outliers)\n",
    "print(pca_d1.shape)\n",
    "pca_d2=pca_data1.pipe(drop_low_outliers)\n",
    "print(pca_d2.shape)\n",
    "pca_data = pca_d2.loc[pca_d2.index.intersection(pca_d1.index)]\n",
    "print('intersection of result')\n",
    "print(pca_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_colonnes_vides, threshold = 0.25\n",
      "(33863, 9)\n",
      "['ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n', 'energy_100g', 'fat_100g', 'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g', 'proteins_100g', 'sodium_100g']\n"
     ]
    }
   ],
   "source": [
    "def list_colonnes_vides(df: pd.DataFrame, threshold=0):\n",
    "    count = df.isna().mean()\n",
    "    colonnes_vides = count[(1 - count) <= threshold].index.to_list()\n",
    "    print(f'list_colonnes_vides, threshold={threshold} (nb cols = {len(colonnes_vides)}) :{colonnes_vides}')\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_colonnes_vides(df, threshold=0):\n",
    "    count = df.isna().mean()\n",
    "    colonnes_vides = count[(1 - count) <= threshold].index.to_list()\n",
    "    print(f'drop_colonnes_vides, threshold = {threshold}')\n",
    "    return df.drop(colonnes_vides, axis=1)\n",
    "pca_data2=pca_data.pipe(drop_colonnes_vides,threshold=0.25)\n",
    "print(pca_data2.shape)\n",
    "print(pca_data.columns.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33863, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_data3=pca_data2\n",
    "# .dropna(subset=['nutriscore_score'])\n",
    "pca_data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33863, 9)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 33863 entries, 3 to 99992\n",
      "Data columns (total 9 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   ingredients_from_palm_oil_n              33863 non-null  Int64  \n",
      " 1   ingredients_that_may_be_from_palm_oil_n  33863 non-null  Int64  \n",
      " 2   energy_100g                              30529 non-null  float64\n",
      " 3   fat_100g                                 30642 non-null  float64\n",
      " 4   saturated-fat_100g                       28710 non-null  float64\n",
      " 5   carbohydrates_100g                       30627 non-null  float64\n",
      " 6   sugars_100g                              29941 non-null  float64\n",
      " 7   proteins_100g                            30602 non-null  float64\n",
      " 8   sodium_100g                              30274 non-null  float64\n",
      "dtypes: Int64(2), float64(7)\n",
      "memory usage: 2.6 MB\n"
     ]
    }
   ],
   "source": [
    "pca_data4=pca_data3\n",
    "# .dropna(subset=['additives_n','salt_100g','fiber_100g'])\n",
    "print(pca_data4.shape)\n",
    "pca_data4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28020 entries, 3 to 99992\n",
      "Data columns (total 9 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   ingredients_from_palm_oil_n              28020 non-null  Int64  \n",
      " 1   ingredients_that_may_be_from_palm_oil_n  28020 non-null  Int64  \n",
      " 2   energy_100g                              28020 non-null  float64\n",
      " 3   fat_100g                                 28020 non-null  float64\n",
      " 4   saturated-fat_100g                       28020 non-null  float64\n",
      " 5   carbohydrates_100g                       28020 non-null  float64\n",
      " 6   sugars_100g                              28020 non-null  float64\n",
      " 7   proteins_100g                            28020 non-null  float64\n",
      " 8   sodium_100g                              28020 non-null  float64\n",
      "dtypes: Int64(2), float64(7)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "pca_data5=pca_data4.dropna()\n",
    "# .drop(columns={'vitamin-a_100g','vitamin-c_100g','calcium_100g','iron_100g','energy-kcal_100g'}).dropna()\n",
    "pca_data5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ingredients_from_palm_oil_n',\n",
      "       'ingredients_that_may_be_from_palm_oil_n', 'energy_100g', 'fat_100g',\n",
      "       'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g',\n",
      "       'proteins_100g', 'sodium_100g'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "features = pca_data5.columns\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': '2524', 'Unnamed: 0.1': '511303', 'code': '891123002176', 'url': 'http://world-en.openfoodfacts.org/product/0891123002176/cranberry-apple-pie', 'creator': 'org-database-usda', 'created_t': Timestamp('2020-04-23 21:13:11'), 'created_datetime': Timestamp('2020-04-23 21:13:11'), 'last_modified_t': Timestamp('2020-04-23 21:13:11'), 'last_modified_datetime': Timestamp('2020-04-23 21:13:11'), 'product_name': 'Cranberry apple pie', 'generic_name': nan, 'quantity': nan, 'packaging': nan, 'packaging_tags': nan, 'brands': nan, 'brands_tags': nan, 'categories': 'Biscuits and cakes, Cakes', 'categories_tags': 'en:biscuits-and-cakes,en:cakes', 'categories_en': 'Biscuits and cakes,Cakes', 'origins': nan, 'origins_tags': nan, 'origins_en': nan, 'manufacturing_places': nan, 'manufacturing_places_tags': nan, 'labels': nan, 'labels_tags': nan, 'labels_en': nan, 'emb_codes': nan, 'emb_codes_tags': nan, 'first_packaging_code_geo': nan, 'cities_tags': nan, 'purchase_places': nan, 'stores': nan, 'countries': 'United States', 'countries_tags': 'en:united-states', 'countries_en': 'United States', 'ingredients_text': 'Sliced apples (apples, ascorbic acid and citric acid to retard oxidation, salt), wheat flour, sugar, water, palm oil, cranberries, butter (cream, salt), food starch-modified, vinegar, egg whites, spices, salt, nonfat dry milk.', 'allergens': 'en:gluten,en:milk', 'traces': nan, 'traces_tags': nan, 'traces_en': nan, 'serving_size': '0.1 PIE (138 g)', 'serving_quantity': '138.0', 'additives_n': 0, 'additives_tags': nan, 'additives_en': nan, 'ingredients_from_palm_oil_n': 0, 'ingredients_that_may_be_from_palm_oil_n': 0, 'ingredients_that_may_be_from_palm_oil_tags': nan, 'nutriscore_score': 12.0, 'nutriscore_grade': 'd', 'nova_group': '3', 'pnns_groups_1': 'Sugary snacks', 'pnns_groups_2': 'Biscuits and cakes', 'states': 'en:to-be-completed, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed, en:packaging-code-to-be-completed, en:characteristics-to-be-completed, en:categories-completed, en:brands-to-be-completed, en:packaging-to-be-completed, en:quantity-to-be-completed, en:product-name-completed, en:photos-to-be-uploaded', 'states_tags': 'en:to-be-completed,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-completed,en:packaging-code-to-be-completed,en:characteristics-to-be-completed,en:categories-completed,en:brands-to-be-completed,en:packaging-to-be-completed,en:quantity-to-be-completed,en:product-name-completed,en:photos-to-be-uploaded', 'states_en': 'To be completed,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Packaging code to be completed,Characteristics to be completed,Categories completed,Brands to be completed,Packaging to be completed,Quantity to be completed,Product name completed,Photos to be uploaded', 'brand_owner': 'Hugo VPM, LLC', 'ecoscore_score_fr': nan, 'ecoscore_grade_fr': nan, 'main_category': 'en:cakes', 'main_category_en': 'Cakes', 'energy-kj_100g': nan, 'energy-kcal_100g': 246.0, 'energy_100g': 1029.0, 'fat_100g': 10.14, 'saturated-fat_100g': 5.07, 'monounsaturated-fat_100g': nan, 'polyunsaturated-fat_100g': nan, 'trans-fat_100g': 0.0, 'cholesterol_100g': 0.0069999999999999, 'carbohydrates_100g': 37.68, 'sugars_100g': 18.84, 'fiber_100g': 1.4, 'proteins_100g': 1.45, 'salt_100g': 0.3075, 'sodium_100g': 0.123, 'vitamin-a_100g': nan, 'vitamin-c_100g': nan, 'potassium_100g': 0.048, 'calcium_100g': 0.006, 'iron_100g': 0.0, 'nutrition-score-fr_100g': 12.0}\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pts_kj_boisson(energy_100g):\n",
    "    # Valeur énergétique (kJ/100g ou kJ/100ml)\n",
    "    limit = [0,30,60,90,120,150,180,210,240,270]\n",
    "    for i in range(10):\n",
    "        if energy_100g <= limit[i]: break;return i\n",
    "    return 10\n",
    "\n",
    "def pts_kj(energy_100g,categ=None):\n",
    "    # Valeur énergétique (kJ/100g ou kJ/100ml)\n",
    "    if categ=='boisson': # Boissons\n",
    "        return pts_kj_boisson(energy_100g)\n",
    "    else: # Cas général, Fromages, Matières grasses ajoutées\n",
    "        limit=[335, 670, 1005, 1340, 1675, 2010, 2345, 2680, 3015, 3350]\n",
    "    for i in range(10):\n",
    "        if energy_100g <= limit[i]: break;return i\n",
    "    return 10\n",
    "\n",
    "\n",
    "def pts_glus_boisson(sugar_100g):\n",
    "    # Sucres (g/100g ou 100mL)\n",
    "    limit=[0,1.5,3,4.5,6,7.5,9,10.5,12,13.5]\n",
    "    for i in range(10):\n",
    "        if sugar_100g <= limit[i]: break;return i\n",
    "    return 10\n",
    "\n",
    "def pts_glus(sugar_100g, categ=None):\n",
    "    # Sucres (g/100g ou 100mL)\n",
    "    if categ=='boisson': # Boissons\n",
    "        return pts_glus_boisson(sugar_100g)\n",
    "    else: # Cas général, Fromages, Matières grasses ajoutées\n",
    "        limit=[4.5,9,13.5,18,22.5,27,5,31,36,40,45]\n",
    "        for i in range(10):\n",
    "            if sugar_100g <= limit[i]: break;return i\n",
    "        return 10\n",
    "\n",
    "def pts_ags_added_fat(saturated_fat_100g, fat_100g):\n",
    "    # Acides gras saturés (g/100g ou 100mL)\n",
    "    # categ Matières grasses ajoutées\n",
    "    limit=[10,16,22,28,34,40,46,52,58,64]\n",
    "    ags_lip_tot=saturated_fat_100g/fat_100g*100\n",
    "    for i in range(10):\n",
    "        if ags_lip_tot <= limit[i]: break;return i\n",
    "    return 10\n",
    "\n",
    "def pts_ags(saturated_fat_100g, categ=None, fat_100g=None):\n",
    "    # Acides gras saturés (g/100g ou 100mL)\n",
    "    if categ=='added_fat': # Matières grasses ajoutées\n",
    "        return pts_ags_added_fat(saturated_fat_100g,fat_100g)\n",
    "\n",
    "    else: # Cas général, Fromages, Boissons\n",
    "        limit=[1,2,3,4,5,6,7,8,9,10]\n",
    "        for i in range(10):\n",
    "            if saturated_fat_100g <= limit[i]: break;return i\n",
    "        return 10\n",
    "\n",
    "\n",
    "\n",
    "def pts_na(sodium_100g):\n",
    "    # Sodium (mg/100g ou 100mL), toutes les catégories\n",
    "    limit=[90,180,270,360,450,540,630,720,810,900]\n",
    "    for i in range(10):\n",
    "        if sodium_100g <= limit[i]: break;return i\n",
    "    return 10\n",
    "\n",
    "def pts_prot(protein_100g):\n",
    "    # Protéines (g/100g ou 100mL), toutes les catégories\n",
    "    limit=[1.6,3.2,4.8,6.4,8]\n",
    "    for i in range(5):\n",
    "        if protein_100g <= limit[i]: break;return i\n",
    "    return 5\n",
    "\n",
    "def pts_fib(fibre_100g):\n",
    "    # Fibres (g/100g ou 100mL), toutes les catégories\n",
    "    limit=[0.9,1.9,2.8,3.7,4.7]\n",
    "    for i in range(5):\n",
    "        if fibre_100g <= limit[i]: break;return i\n",
    "    return 5\n",
    "\n",
    "\n",
    "def pts_FLN_boisson(FLN_100g):\n",
    "    # Fruits, légumes, légumineuses, fruits à coques, huiles de colza, de noix et d'olive (%/100g ou 100mL)\n",
    "    limit=[40,60,80]\n",
    "    for i in range(3):\n",
    "        if FLN_100g<limit[i]: break; return i*2\n",
    "    return 10\n",
    "\n",
    "def pts_FLN(FLN_100g, categ=None):\n",
    "    # Fruits, légumes, légumineuses, fruits à coques, huiles de colza, de noix et d'olive (%/100g ou 100mL)\n",
    "    if categ=='boisson': return pts_FLN_boisson()\n",
    "    # pas boisson\n",
    "    limit=[40,60,80]    \n",
    "    for i in range(3):\n",
    "        if FLN_100g<limit[i]: break; return i\n",
    "    return 5\n",
    "\n",
    "\n",
    "def find_products_by_name(df,chaine, nb=1):\n",
    "    # get row from barcode:\n",
    "    filtre = df['product_name'].notnull()\n",
    "    filtre &= df['product_name'].str.contains(chaine, regex=True, case=True)\n",
    "    filtre &= df['nutriscore_grade'].notnull()\n",
    "    products = df[filtre].sort_values(by=\"product_name\")\n",
    "    if len(products)==0: return None\n",
    "    return products.head(nb)\n",
    "\n",
    "def nutriscore_from_name(df,product_name):\n",
    "    return find_products_by_name(df,product_name,nb=1)\n",
    "\n",
    "def nutriscore(series:pd.DataFrame):\n",
    "    a = b = c = d = 0\n",
    "\n",
    "    # x=find_products_by_name(df,product_name,nb=1)\n",
    "    x=series.to_dict(orient='records')[0]\n",
    "    print (x)\n",
    "\n",
    "    categ=None\n",
    "    # get row from name:\n",
    "    \n",
    "    quantity=x['quantity']\n",
    "    print (quantity)\n",
    "    if pd.isna(quantity):\n",
    "        pass\n",
    "    else:    \n",
    "        if 'ml' in quantity:\n",
    "            categ='boisson'\n",
    " \n",
    "    # points A \n",
    "    energy_100g =x['energy_100g'] #Valeur énergétique (kJ/100g ou kJ/100ml)\n",
    "    sugars_100g =x['sugars_100g'] #Valeur énergétique (kJ/100g ou kJ/100ml)\n",
    "    saturated_fat_100g= x['saturated-fat_100g']\n",
    "    sodium_100g=x['sodium_100g']\n",
    "    if pd.isna(sodium_100g):\n",
    "        salt_100g=x['salt_100g']\n",
    "        sodium_100g= salt_100g/2.5*1000    \n",
    "    fat_100g=x['fat_100g'] \n",
    "    if pd.isna(fat_100g) or pd.isna(saturated_fat_100g):\n",
    "        pass\n",
    "    elif fat_100g > 10 and saturated_fat_100g < fat_100g:\n",
    "        categ='added_fat'\n",
    "\n",
    "    # FLN_100g=x['fruits-vegetables-nuts_100g'] ## presque toujours vide\n",
    "    # if pd.isna(FLN_100g.any()):\n",
    "    FLN_100g=0 #sinon on ne peut rien calculé\n",
    "    proteins_100g=x['proteins_100g']\n",
    "    fiber_100g=x['fiber_100g']\n",
    "\n",
    "    # for nutriscore, all but(salt and fat) must be present--> ptsA= pts_energy+pts_sugar+pts_sat_fats+ pts_sodium\n",
    "    reqd=energy_100g+sugars_100g+saturated_fat_100g+sodium_100g\n",
    "    + proteins_100g+fiber_100g\n",
    "    if(pd.isna(reqd)):\n",
    "        # ne perd pas de temps pour le calcule\n",
    "        # on doit faire le filtrage avant d'appeler ce procedure\n",
    "        ret={'e':energy_100g,'s':sugars_100g,'f':saturated_fat_100g,'Na':sodium_100g,'prot':proteins_100g,'fib':fiber_100g}\n",
    "        return (ret)\n",
    "    else:    \n",
    "        a=pts_kj(energy_100g,categ)\n",
    "        b=pts_glus(sugars_100g,categ)\n",
    "        c=pts_ags(saturated_fat_100g,categ,fat_100g)\n",
    "        d=pts_na(sodium_100g)\n",
    "    \n",
    "    pts_a =a+b+c+d\n",
    "\n",
    "\n",
    "    V=pts_a,\n",
    "    S=pts_prot(proteins_100g)\n",
    "    T=pts_fib(fiber_100g)\n",
    "    U=pts_FLN(FLN_100g)\n",
    "    pts_b=S+T+U\n",
    "    # deja testé si STU remplis\n",
    "    if categ=='fromage':\n",
    "        score = pts_a - pts_b\n",
    "    # cas général\n",
    "    elif pts_a <11:\n",
    "        score= pts_a - pts_b\n",
    "    elif pts_a>=11 and U==5:\n",
    "        # fruits très sucrés/salés, noix etc\n",
    "        # subtract proteins\n",
    "        score=pts_a-pts_b\n",
    "    else:\n",
    "        # Pas besoin de subtraire les proteins\n",
    "        score= pts_a-U-T\n",
    "    return score\n",
    "\n",
    "ap = find_products_by_name(cleaned_data,'apple pie',nb=1)\n",
    "# ap['score'] = nutriscore(ap)\n",
    "nutriscore(ap)\n",
    "# ap['score'].values[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA étape 3  -  standardise (centrage et réduction d'echelle)\n",
    "Variables sur des échelles differentes sont standardisé pour contribuer egalement au analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca_standardise(df):\n",
    "    \"\"\"Standardize the data\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    # return scaler.fit(df).transform(df) # numpy array\n",
    "    return pd.DataFrame(scaler.fit_transform(df),columns=df.columns)\n",
    "\n",
    "X_scaled_df = pca_data5.pipe(pca_standardise)\n",
    "X_scaled_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(df):\n",
    "    \"\"\"\n",
    "        applique le z-score en utilisant .mean() et .std()\n",
    "        même résultat que utilisant StandardScaler\n",
    "        StandardScaler est optimisé pour des grand matrices\n",
    "    \"\"\"\n",
    "    df_std = df.copy()\n",
    "    for column in df_std.columns:\n",
    "        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std(ddof=0)       \n",
    "    return df_std\n",
    "    \n",
    "# call the z_score function\n",
    "X_scaled_df2 = z_score(pca_data5)\n",
    "\n",
    "X_scaled_df2.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_df= cleaned_data[['nova_group']].join(X_scaled_df2,how='inner')\n",
    "target_df=target_df.rename_axis('id').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choix du nombre de composantes à calculer\n",
    "n_comp = 6\n",
    "\n",
    "# Create the PCA model\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=X_scaled_df2.shape[1])\n",
    "# Fit the model with the standardised data\n",
    "pca.fit(X_scaled_df2)\n",
    "\n",
    "nwD=pca.transform(X_scaled_df2)\n",
    "pcs = pca.components_ \n",
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "nb_colonnes=len(pca_data5.columns)\n",
    "dpca = PCA(n_components=nb_colonnes)\n",
    "pc = dpca.fit_transform(X_scaled_df2)\n",
    "nom_cols= [f'PC{i}' for i in range(1,nb_colonnes+1)]\n",
    "print (nom_cols)\n",
    "pc_df = pd.DataFrame(data = pc , columns = nom_cols)\n",
    "pc_df.index=X_scaled_df2.index\n",
    "pc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_visual=target_df[['id','nova_group']].join(pc_df,how='inner')\n",
    "for_visual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_visual=pc_df.join(cleaned_data,how='inner')\n",
    "for_visual[for_visual['PC2']>10].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additives_data=pd.read_csv(os_path_join(OUT_FOLDER,ADDITIVES_DETAIL), encoding='UTF-8', sep='\\t')\n",
    "additives_data.info(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additives_data['efsa_evaluation_overexposure_risk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unsafe_additives(df, tags=('high','moderate')):\n",
    "    if type(tags) is str: tags=[tags]\n",
    "    tags = '|'.join(tags)\n",
    "    filtre = df['efsa_evaluation_overexposure_risk'].fillna('').str.contains(tags, regex=True)\n",
    "    return df[filtre]\n",
    "\n",
    "additives_data.pipe(get_unsafe_additives,'high')\n",
    "additives_data.pipe(get_unsafe_additives,'moderate')\n",
    "unsafe_additives = additives_data.pipe(get_unsafe_additives)\n",
    "print(unsafe_additives['name'].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safe_additives(df, tags=('high','moderate')):\n",
    "    tags = 'high|moderate'\n",
    "    filtre = df['efsa_evaluation_overexposure_risk'].fillna('').str.contains(tags, regex=True)\n",
    "    return df[~filtre]\n",
    "\n",
    "safe_additives = additives_data.pipe(get_safe_additives)\n",
    "print(len(safe_additives))\n",
    "safe_additives['name']\n",
    "# safe_additives_en=safe_additives['name'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutriscore_palette = {'a':'#038141', 'b':'#85bb2f', 'c':'#fecb02', 'd':'#ee8100', 'e':'#e63e11'}\n",
    "ecoscore_palette = {'a':'#1e8f4e', 'b':'#2ecc71', 'c':'#f5c100', 'd':'#ef7e1a', 'e':'#d93726'}\n",
    "nova_palette = {'1': '#00aa00','2': '#ffcc00','3': '#ff6600','4': '#ff0000'}\n",
    "siga_palette = {'A': '#009c3c','B': '#99c336','C': '#008dd1','D': '#00629e'}\n",
    "\n",
    "groups=list(nova_palette.keys())\n",
    "colors=list(nova_palette.values())\n",
    "print(groups)\n",
    "print(colors)\n",
    "\n",
    "def get_palette(feature):\n",
    "    if feature=='nutriscore_grade': return nutriscore_palette \n",
    "    elif feature=='nova_score': return nova_palette \n",
    "    elif feature=='eco_score' or feature=='ecoscore_grade_fr': return ecoscore_palette \n",
    "    else : return None \n",
    "\n",
    "def explode_series(series:pd.Series) -> pd.Series:\n",
    "    \"\"\"convertir ['a,b,c','d,e','f']--> ['a','b','c','d','e','f']  \"\"\"\n",
    "    if series.str.contains(',').any():\n",
    "        return series.str.split(',').explode()\n",
    "    else:\n",
    "        return series.explode()\n",
    "        # return series\n",
    "\n",
    "def get_palette_df(df,feature,nb=10,exclude_values=None):\n",
    "    \"\"\"return dataframe[key,color]\"\"\"\n",
    "    palette=get_palette(feature)\n",
    "    if palette is None:   \n",
    "        # if df[feature].dtype.kind in 'biufc':return None # value_counts\n",
    "        if exclude_values is None: exclude_values=[]\n",
    "        print(f'{feature}, exclude nb: {len(exclude_values)}')\n",
    "        series = explode_series(df[feature]).copy()\n",
    "        filtre=series.isin(exclude_values)\n",
    "        # for val in exclude_values:\n",
    "        #     series = series[series.index!=val]\n",
    "        print(f'exclude count={len(exclude_values)}')\n",
    "        print(f'exclude index count={len(series[filtre])}')\n",
    "        series=series[~filtre]\n",
    "        groups=series.value_counts().head(nb).index\n",
    "        nb_features=min(len(groups),nb)\n",
    "        groups = groups[:nb_features]\n",
    "        if nb_features<11:\n",
    "            colors = sns.color_palette(\"tab10\").as_hex()[:nb_features]\n",
    "        else:    \n",
    "            colors = sns.color_palette(\"tab20\").as_hex()[:nb_features]\n",
    "        ret=pd.DataFrame(groups,columns={'group'})\n",
    "        ret['color']=colors\n",
    "    else:\n",
    "        ret=pd.DataFrame.from_dict(palette,orient='index',columns={'color'}).rename_axis('group').sort_index().reset_index()\n",
    "    return ret\n",
    "\n",
    "\n",
    "safe_additives_en=safe_additives['name'].tolist()\n",
    "\n",
    "print(cleaned_data.pipe(get_palette_df,'nutriscore_grade'))\n",
    "print(cleaned_data.pipe(get_palette_df,'ecoscore_grade_fr'))\n",
    "print(cleaned_data.pipe(get_palette_df,'pnns_groups_1',exclude_values=['unknown','Alcoholic beverages']))\n",
    "print(cleaned_data.pipe(get_palette_df,'additives_en',exclude_values=safe_additives['name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "convert_string_to_list = lambda x: x.str.split(',') if x else x\n",
    "\n",
    "def explode_by_series(df,col,exclude_groups=None):\n",
    "    # si le colonne est numérique, pas besoin de traitement\n",
    "    if df[col].dtype.kind in 'biufc':return df\n",
    "    ret=df.dropna().copy()\n",
    "    ret[col]= ret[col].str.split(',')\n",
    "    ret=ret.explode(col)\n",
    "    return ret\n",
    "\n",
    "feature='additives_tags'\n",
    "feature='nutriscore_grade'\n",
    "explode_by_series(for_visual[['PC1','PC2',feature]],feature).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_components(data,axis1='PC1',axis2='PC2',feature='nutriscore_grade',exclude_groups=None):\n",
    "    df=explode_by_series(data[[axis1,axis2,feature]],feature,exclude_groups)\n",
    "    # return df.shape\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel(axis1, fontsize = 15)\n",
    "    ax.set_ylabel(axis2, fontsize = 15)\n",
    "    ax.set_title(f'Principal Components ({axis1}, {axis2}) vs. {feature}', fontsize = 20)\n",
    "    palette = get_palette_df(df,feature,exclude_values=exclude_groups)\n",
    "    groups = palette['group']\n",
    "    colors = palette['color']\n",
    "    for group, color in zip(groups,colors):\n",
    "        if (group!='unknown'): \n",
    "            indicesToKeep = df[feature] == group\n",
    "            ax.scatter(df.loc[indicesToKeep, axis1],\n",
    "                      df.loc[indicesToKeep, axis2],\n",
    "                        c = color, alpha=0.5,s = 1)\n",
    "    # ax.legend(groups,bbox_to_anchor=(1.2,0.5),loc=\"center right\", fontsize=12, \n",
    "            # bbox_transform=plt.gcf().transFigure)\n",
    "    ax.legend(groups,markerscale=6)\n",
    "    ax.grid()\n",
    "    # plt.subplots_adjust(left=0.0, bottom=0.1, right=0.4)\n",
    "    plt.subplots_adjust()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_components(for_visual, axis1='PC1',axis2='PC2',feature='nutriscore_grade')\n",
    "plot_components(for_visual, axis1='PC1',axis2='PC2',feature='ecoscore_grade_fr')\n",
    "plot_components(for_visual, axis1='PC1',axis2='PC2',feature='pnns_groups_1',exclude_groups=['unknown','Alcoholic beverages'])\n",
    "# plot_components(for_visual, axis1='PC1',axis2='PC2',feature='additives_n')\n",
    "plot_components(for_visual, axis1='PC1',axis2='PC2',feature='additives_en',exclude_groups=safe_additives['name'])\n",
    "plot_components(for_visual, axis1='PC1',axis2='PC2',feature='additives_tags',exclude_groups=safe_additives['id'])\n",
    "plot_components(for_visual, axis1='PC3',axis2='PC4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_visual[for_visual['PC2']>20]['product_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'var':dpca.explained_variance_ratio_,\n",
    "             'PC':nom_cols})\n",
    "sns.barplot(x='PC',y=\"var\", \n",
    "           data=df, color=\"c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree plot\n",
    "A visual approach to selecting the number of principal components to keep means the use of a scree plot. A scree plot shows the number of components on the X-axis against the proportion of the variance explained on the Y-axis. The suggested number of components to keep is where the plot forms an elbow and the curve flattens out. Unfortunately, the scree plot often presents some ambiguity.  Further, a practical approach often prompts analysts to evaluate the first few principal components, and if they are of interest, the analyst would continue considering additional principal components.  However, if the first few principal components provide little relevance, evaluation of additional principal components is likely of no use.\n",
    "\n",
    "The Kaiser rule suggests the minimum eigenvalue rule.  In this case, the number of principal components to keep equals the number of eigenvalues greater than 1.\n",
    "\n",
    "Finally, the number of components to keep could be determined by a minimal threshold that explains variation in the data. In this case, we would keep as many principal components as needed to explain at least 70% (or some other threshold) of the total variation in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def display_scree_plot(pca):\n",
    "    '''Display a scree plot for the pca'''\n",
    "    # scree = pca.explained_variance_\n",
    "    scree = pca.explained_variance_ratio_*100\n",
    "    plt.bar(np.arange(len(scree))+1, scree)\n",
    "    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    # plt.xlabel('Principal Component')\n",
    "    # plt.ylabel('Eigenvalue')\n",
    "    plt.axhline(y=1, linewidth=1, color='r', alpha=0.5)\n",
    "    plt.title('Scree Plot of PCA: Component Eigenvalues')\n",
    "\n",
    "    plt.xlabel(\"Number of principal components\")\n",
    "    plt.ylabel(\"Percentage explained variance\")\n",
    "    plt.title(\"Scree plot\")\n",
    "    plt.show(block=False)\n",
    "\n",
    "display_scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_explained(pca):\n",
    "    import numpy as np\n",
    "    from matplotlib.pyplot import figure, show\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "    # ax = figure().gca()\n",
    "    x=np.arange(1,len(pca.explained_variance_ratio_)+1)\n",
    "    y=np.cumsum(pca.explained_variance_ratio_)\n",
    "    print(f'x = {len(x)}; y = {len(y)}')\n",
    "\n",
    "    ax=sns.lineplot(x=x,y=y,marker='+')\n",
    "\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.axvline(x=8, linewidth=1, color='r', alpha=0.5)\n",
    "    plt.axhline(y=0.9, linewidth=1, color='r', alpha=0.5)\n",
    "    plt.title('Explained Variance of PCA by Component')\n",
    "    show()\n",
    "\n",
    "var_explained(dpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cercle des corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_circles(pcs, n_comp, pca, axis_ranks, labels=None, label_rotation=0, lims=None):\n",
    "    for d1, d2 in axis_ranks: # On affiche les 3 premiers plans factoriels, donc les 6 premières composantes\n",
    "        if d2 < n_comp:\n",
    "\n",
    "            # initialisation de la figure\n",
    "            fig, ax = plt.subplots(figsize=(7,6))\n",
    "\n",
    "            # détermination des limites du graphique\n",
    "            if lims is not None :\n",
    "                xmin, xmax, ymin, ymax = lims\n",
    "            elif pcs.shape[1] < 30 :\n",
    "                xmin, xmax, ymin, ymax = -1, 1, -1, 1\n",
    "            else :\n",
    "                xmin, xmax, ymin, ymax = min(pcs[d1,:]), max(pcs[d1,:]), min(pcs[d2,:]), max(pcs[d2,:])\n",
    "\n",
    "            # affichage des flèches\n",
    "            # s'il y a plus de 30 flèches, on n'affiche pas le triangle à leur extrémité\n",
    "            if pcs.shape[1] < 30 :\n",
    "                plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n",
    "                   pcs[d1,:], pcs[d2,:], \n",
    "                   angles='xy', scale_units='xy', scale=1, color=\"grey\")\n",
    "                # (voir la doc : https://matplotlib.org/api/_as_gen/matplotlib.pyplot.quiver.html)\n",
    "            else:\n",
    "                lines = [[[0,0],[x,y]] for x,y in pcs[[d1,d2]].T]\n",
    "                ax.add_collection(LineCollection(lines, axes=ax, alpha=.1, color='black'))\n",
    "            \n",
    "            # affichage des noms des variables  \n",
    "            if labels is not None:  \n",
    "                for i,(x, y) in enumerate(pcs[[d1,d2]].T):\n",
    "                    if x >= xmin and x <= xmax and y >= ymin and y <= ymax :\n",
    "                        plt.text(x, y, labels[i], fontsize='14', ha='center', va='center', rotation=label_rotation, color=\"blue\", alpha=0.5)\n",
    "            \n",
    "            # affichage du cercle\n",
    "            circle = plt.Circle((0,0), 1, facecolor='none', edgecolor='b')\n",
    "            plt.gca().add_artist(circle)\n",
    "\n",
    "            # définition des limites du graphique\n",
    "            plt.xlim(xmin, xmax)\n",
    "            plt.ylim(ymin, ymax)\n",
    "        \n",
    "            # affichage des lignes horizontales et verticales\n",
    "            plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "            plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "            # nom des axes, avec le pourcentage d'inertie expliqué\n",
    "            plt.xlabel(f'PC{d1+1} ({round(100*pca.explained_variance_ratio_[d1],1)}%)')\n",
    "            plt.ylabel(f'PC{d2+1} ({round(100*pca.explained_variance_ratio_[d2],1)}%)')\n",
    "\n",
    "            plt.title(\"Cercle des corrélations (PC{} et PC{})\".format(d1+1, d2+1))\n",
    "            plt.show(block=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pcs = dpca.components_\n",
    "display_circles(pcs, n_comp, dpca, [(0,1),(2,3),(4,5)], labels = np.array(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_circles_again(pcs, n_comp, pca, axis_ranks, labels=None, label_rotation=0, lims=None):\n",
    "    \"\"\"Display correlation circles, one for each factorial plane\"\"\"\n",
    "\n",
    "    # For each factorial plane\n",
    "    for d1, d2 in axis_ranks: \n",
    "        if d2 < n_comp:\n",
    "\n",
    "            # Initialise the matplotlib figure\n",
    "            fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "            # Determine the limits of the chart\n",
    "            if lims is not None :\n",
    "                xmin, xmax, ymin, ymax = lims\n",
    "            elif pcs.shape[1] < 30 :\n",
    "                xmin, xmax, ymin, ymax = -1, 1, -1, 1\n",
    "            else :\n",
    "                xmin, xmax, ymin, ymax = min(pcs[d1,:]), max(pcs[d1,:]), min(pcs[d2,:]), max(pcs[d2,:])\n",
    "\n",
    "            # Add arrows\n",
    "            # If there are more than 30 arrows, we do not display the triangle at the end\n",
    "            if pcs.shape[1] < 30 :\n",
    "                plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n",
    "                   pcs[d1,:], pcs[d2,:], \n",
    "                   angles='xy', scale_units='xy', scale=1, color=\"grey\")\n",
    "                # (see the doc : https://matplotlib.org/api/_as_gen/matplotlib.pyplot.quiver.html)\n",
    "            else:\n",
    "                lines = [[[0,0],[x,y]] for x,y in pcs[[d1,d2]].T]\n",
    "                ax.add_collection(LineCollection(lines, axes=ax, alpha=.1, color='black'))\n",
    "            \n",
    "            # Display variable names\n",
    "            if labels is not None:  \n",
    "                for i,(x, y) in enumerate(pcs[[d1,d2]].T):\n",
    "                    if x >= xmin and x <= xmax and y >= ymin and y <= ymax :\n",
    "                        plt.text(x, y, labels[i], fontsize='14', ha='center', va='center', rotation=label_rotation, color=\"blue\", alpha=0.5)\n",
    "            \n",
    "            # Display circle\n",
    "            circle = plt.Circle((0,0), 1, facecolor='none', edgecolor='b')\n",
    "            plt.gca().add_artist(circle)\n",
    "\n",
    "            # Define the limits of the chart\n",
    "            plt.xlim(xmin, xmax)\n",
    "            plt.ylim(ymin, ymax)\n",
    "        \n",
    "            # Display grid lines\n",
    "            plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "            plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "            # Label the axes, with the percentage of variance explained\n",
    "            plt.xlabel('PC{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n",
    "            plt.ylabel('PC{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n",
    "            nr=d1+1\n",
    "            plt.title(\"Correlation Circle (PC{} and PC{})\".format(d1+1, d2+1))\n",
    "            plt.show(block=False)\n",
    "            d = {'values': pca.components_[d1], 'factors': labels}\n",
    "            df1= pd.DataFrame(d)\n",
    "            df1.set_index('factors')\n",
    "            df2=df1.sort_values(by='values', ascending=False)\n",
    "            df3=df1.sort_values(by='values', ascending=True)\n",
    "            print(\"Principal Component\" + str(nr)+ \" Presenting Values\")\n",
    "            print(df2.head(3))\n",
    "            print(df3.head(3))\n",
    "            \n",
    "            nr=d2+1\n",
    "            \n",
    "            d = {'values': pca.components_[d2], 'factors': labels}\n",
    "            df1= pd.DataFrame(d)\n",
    "            df1.set_index('factors')\n",
    "            df2=df1.sort_values(by='values', ascending=False)\n",
    "            df3=df1.sort_values(by='values', ascending=True)\n",
    "            print(\"Principal Component\" + str(nr)+ \" Presenting Values\")\n",
    "            print(df2.head(3))\n",
    "            print(df3.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_factorial_planes(X_projected, n_comp, pca, axis_ranks, labels=None, alpha=1, illustrative_var=None):\n",
    "    '''Display a scatter plot on a factorial plane, one for each factorial plane'''\n",
    "\n",
    "    # For each factorial plane\n",
    "    for d1,d2 in axis_ranks:\n",
    "        if d2 < n_comp:\n",
    " \n",
    "            # Initialise the matplotlib figure      \n",
    "            fig = plt.figure(figsize=(7,6))\n",
    "        \n",
    "            # Display the points\n",
    "            if illustrative_var is None:\n",
    "                plt.scatter(X_projected[:, d1], X_projected[:, d2], alpha=alpha,s=1)\n",
    "            else:\n",
    "                illustrative_var = np.array(illustrative_var)\n",
    "                for value in np.unique(illustrative_var):\n",
    "                    selected = np.where(illustrative_var == value)\n",
    "                    plt.scatter(X_projected[selected, d1], X_projected[selected, d2], alpha=alpha, label=value, s=1)\n",
    "                plt.legend()\n",
    "\n",
    "            # Display the labels on the points\n",
    "            if labels is not None:\n",
    "                for i,(x,y) in enumerate(X_projected[:,[d1,d2]]):\n",
    "                    plt.text(x, y, labels[i],\n",
    "                              fontsize='14', ha='center',va='center') \n",
    "                \n",
    "            # Define the limits of the chart\n",
    "            boundary = np.max(np.abs(X_projected[:, [d1,d2]])) * 1.1\n",
    "            plt.xlim([-boundary,boundary])\n",
    "            plt.ylim([-boundary,boundary])\n",
    "        \n",
    "            # Display grid lines\n",
    "            plt.plot([-100, 100], [0, 0], color='grey', ls='--')\n",
    "            plt.plot([0, 0], [-100, 100], color='grey', ls='--')\n",
    "\n",
    "            # Label the axes, with the percentage of variance explained\n",
    "            plt.xlabel('PC{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n",
    "            plt.ylabel('PC{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n",
    "\n",
    "            plt.title(\"Projection of points (on PC{} and PC{})\".format(d1+1, d2+1))\n",
    "            #plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection des individus\n",
    "X_projected = pca.transform(X_scaled_df2)\n",
    "display_factorial_planes(X_projected, n_comp, dpca, [(0,1),(2,3),(4,5)], labels = None)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2: Clustering via K Nearest Neighbours\n",
    "K-means : unsupervised --> creates (new) classes if a new feature is needed (eg if wish to do KNN but don't have any labels yet)\n",
    "KNN : supervised --> after training, can attribute unlabeled individuals to an existing label (class)\n",
    "\n",
    "- Explanation\n",
    "  - <https://towardsdatascience.com/getting-acquainted-with-k-nearest-neighbors-ba0a9ecf354f>\n",
    "  - <https://www.datasciencecentral.com/profiles/blogs/k-nearest-neighbor-algorithm-using-python>\n",
    "    - The kNN task can be broken down into writing 3 primary functions:\n",
    "        1. Calculate the distance between any two points\n",
    "        2. Find the nearest neighbours based on these pairwise distances\n",
    "        3. Majority vote on a class labels based on the nearest neighbour list\n",
    "  - <https://stats.stackexchange.com/questions/56500/what-are-the-main-differences-between-k-means-and-k-nearest-neighbours>\n",
    "\n",
    "- Imputation\n",
    "  - <https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning/>\n",
    "    - with train test verification before applying the imputation\n",
    "  - <https://medium.com/@kyawsawhtoon/a-guide-to-knn-imputation-95e2dc496e>\n",
    "\n",
    "- visualisation\n",
    "  - <https://towardsdatascience.com/knn-visualization-in-just-13-lines-of-code-32820d72c6b6>\n",
    "- using sparse matrices (500000 columns!!) and cosine similarity\n",
    "  - <https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-1-knn-item-based-collaborative-filtering-637969614ea>\n",
    "\n",
    "- Movie Recommender System Using K-Means Clustering AND K-Nearest Neighbor\n",
    "  - <https://ieeexplore.ieee.org/document/8776969>\n",
    "  - <https://github.com/BenChristensen12/movie-recommender-system>\n",
    "\n",
    "- Step-by-step (movie) recommendation system based on tags similarity (actors, genres, description)\n",
    "  - <https://www.analyticsvidhya.com/blog/2020/08/recommendation-system-k-nearest-neighbors/>\n",
    "  - <https://www.kaggle.com/heeraldedhia/movie-ratings-and-recommendation-using-knn>\n",
    "  - could replace with tag column similarities (brand, categories_en, ingredients)\n",
    "    input label name --> find product with closest name, then use recommender to recommend similar products\n",
    "\n",
    "- Pipelines\n",
    "  - <https://stackoverflow.com/questions/50335203/how-to-apply-knn-on-a-mixed-datasetnumerical-categorical-after-doing-one-hot>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN step 1 - Divide Data Into Features and Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(cleaned_data.columns))\n",
    "print(list(pca_data5.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown_values(df, col='pnns_groups_1'):\n",
    "    # series = df[[col]].copy()\n",
    "    # series.replace('unknown',np.NaN)\n",
    "    # df.loc[:,col]=series.loc[:,col]\n",
    "    df.loc[:,col] = df.loc[:,col].replace('unknown', np.NaN)\n",
    "    # df[col].map(lambda x: np.nan if x==\"unknown\"  else x)\n",
    "    return df\n",
    "\n",
    "sel_cols=pca_data5.columns.to_list()\n",
    "label_cols=['nova_group', 'pnns_groups_1','ingredients_text','additives_n','nutriscore_score','nutriscore_grade','nutrition-score-fr_100g']\n",
    "# anal_cols=sel_cols+label_cols\n",
    "\n",
    "anal_cols= [\n",
    "     'energy_100g', 'fat_100g', 'carbohydrates_100g', 'proteins_100g', 'sugars_100g','saturated-fat_100g','sodium_100g', \n",
    "     \n",
    "     'nutriscore_score', 'nutriscore_grade', 'nutrition-score-fr_100g',\n",
    "      'ingredients_text','ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n', 'additives_n',  \n",
    "       'nova_group',   \n",
    "      'main_category_en', 'categories_en',\n",
    "      'pnns_groups_1', 'pnns_groups_2',\n",
    "      'ecoscore_grade_fr', 'brands'\n",
    "      ]\n",
    "to_analyse=cleaned_data[anal_cols].pipe(replace_unknown_values,'pnns_groups_1').pipe(replace_unknown_values,'pnns_groups_2')\n",
    "\n",
    "to_analyse.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "to_analyse=to_analyse.sort_values(by=['energy_100g', 'fat_100g', 'carbohydrates_100g', 'proteins_100g', 'sugars_100g','sodium_100g', \n",
    "     'saturated-fat_100g',\n",
    "     'nutriscore_score', 'nutriscore_grade',\n",
    "      'ingredients_text','ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n', 'additives_n',\n",
    "      'pnns_groups_1',\n",
    "           'nova_group'])\n",
    "to_analyse=to_analyse.sort_values(by=['nutriscore_grade','ingredients_that_may_be_from_palm_oil_n',\n",
    "'pnns_groups_1','main_category_en','nova_group',\n",
    "'sodium_100g','fat_100g','energy_100g',])\n",
    "msno.matrix(to_analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_analyse=to_analyse.sort_values(by=['ingredients_that_may_be_from_palm_oil_n','nutriscore_grade',\n",
    "'pnns_groups_1','main_category_en','nova_group',\n",
    "'energy_100g','sodium_100g','fat_100g',])\n",
    "msno.matrix(to_analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "X = pca_data5[feature_columns].values\n",
    "y = dataset['Species'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets, neighbors\n",
    "# from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "def knn_comparison(data, k):\n",
    "    x = data[['X','Y']].values\n",
    "    y = data['class'].astype(int).values\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(x, y)\n",
    "    # Plotting decision region\n",
    "    # plot_decision_regions(x, y, clf=clf, legend=2)\n",
    "    # Adding axes annotations\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title(f'Knn with K={ str(k)}')\n",
    "    plt.show()\n",
    "\n",
    "# data1 = pd.read_csv('ushape.csv')\n",
    "# for i in [1,5,20,30,40,80]:\n",
    "    # knn_comparison(data1, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_class(df, class_name, feature, thresholds, names):\n",
    "    '''Append a new class feature named 'class_name' based on a threshold split of 'feature'.  \n",
    "    Threshold values are in 'thresholds' and class names are in 'names'.'''\n",
    "    \n",
    "    n = pd.cut(df[feature], bins = thresholds, labels=names)\n",
    "    df[class_name] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(Z, names, figsize=(10,25)):\n",
    "    '''Plot a dendrogram to illustrate hierarchical clustering'''\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    plt.xlabel('distance')\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        labels = names,\n",
    "        orientation = \"left\",\n",
    "    )\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "def display_parallel_coordinates(df, num_clusters):\n",
    "    '''Display a parallel coordinates plot for the clusters in df'''\n",
    "\n",
    "    # Select data points for individual clusters\n",
    "    cluster_points = []\n",
    "    for i in range(num_clusters):\n",
    "        cluster_points.append(df[df.cluster==i])\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(12, 15))\n",
    "    title = fig.suptitle(\"Parallel Coordinates Plot for the Clusters\", fontsize=18)\n",
    "    fig.subplots_adjust(top=0.95, wspace=0)\n",
    "\n",
    "    # Display one plot for each cluster, with the lines for the main cluster appearing over the lines for the other clusters\n",
    "    for i in range(num_clusters):    \n",
    "        plt.subplot(num_clusters, 1, i+1)\n",
    "        for j,c in enumerate(cluster_points): \n",
    "            if i!= j:\n",
    "                pc = parallel_coordinates(c, 'cluster', color=[addAlpha(palette[j],0.2)])\n",
    "                pc = parallel_coordinates(cluster_points[i], 'cluster', color=[addAlpha(palette[i],0.5)])\n",
    "\n",
    "        # Stagger the axes\n",
    "        ax=plt.gca()\n",
    "        for tick in ax.xaxis.get_major_ticks()[1::2]:\n",
    "            tick.set_pad(20)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_parallel_coordinates_centroids(df, num_clusters):\n",
    "    '''Display a parallel coordinates plot for the centroids in df'''\n",
    "\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    title = fig.suptitle(\"Parallel Coordinates plot for the Centroids\", fontsize=18)\n",
    "    fig.subplots_adjust(top=0.9, wspace=0)\n",
    "\n",
    "    # Draw the chart\n",
    "    parallel_coordinates(df, 'cluster', color=palette)\n",
    "\n",
    "    # Stagger the axes\n",
    "    ax=plt.gca()\n",
    "    for tick in ax.xaxis.get_major_ticks()[1::2]:\n",
    "        tick.set_pad(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 3 : ANOVA"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0c4969017d99d386ec3e08191b6f994618d2885196bfce758f351869385c277"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('OC_2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
